{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhH39GExX-HK",
        "outputId": "c079465b-fc3b-4ac5-bd4d-19fbe4757237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.0156 - mae: 0.0657 - val_loss: 0.0031 - val_mae: 0.0489\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0081 - mae: 0.0581 - val_loss: 0.0021 - val_mae: 0.0363\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0066 - mae: 0.0404 - val_loss: 0.0021 - val_mae: 0.0315\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0056 - mae: 0.0395 - val_loss: 0.0016 - val_mae: 0.0291\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0052 - mae: 0.0375 - val_loss: 0.0014 - val_mae: 0.0266\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0075 - mae: 0.0383 - val_loss: 0.0016 - val_mae: 0.0284\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0034 - mae: 0.0310 - val_loss: 0.0014 - val_mae: 0.0269\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0044 - mae: 0.0364 - val_loss: 0.0014 - val_mae: 0.0239\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0027 - mae: 0.0282 - val_loss: 0.0018 - val_mae: 0.0300\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0029 - mae: 0.0320 - val_loss: 0.0015 - val_mae: 0.0279\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0015 - mae: 0.0282\n",
            "Test Loss: 0.0014630577061325312, Test MAE: 0.027949267998337746\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "           channel_info  predicted_influence  rank        country\n",
            "95                  433             0.097945   8.0    Netherlands\n",
            "15          virat.kohli             0.201024   3.0            NaN\n",
            "30   chrisbrownofficial             0.139144   4.0  United States\n",
            "158       chrissyteigen             0.051914  24.0  United States\n",
            "128         luissuarez9             0.035449  33.0            NaN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-f394b1e03d38>:93: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test['predicted_influence'] = y_pred\n",
            "<ipython-input-12-f394b1e03d38>:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test['rank'] = df_test['predicted_influence'].rank(ascending=False)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Download stopwords if not already done\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the Instagram dataset\n",
        "df = pd.read_csv('/content/sample_data/top_insta_influencers_data.csv')  # Replace with your actual dataset path\n",
        "\n",
        "# Function to convert abbreviated numbers to floats\n",
        "def convert_abbreviated_numbers(value):\n",
        "    if isinstance(value, str):\n",
        "        if 'k' in value:\n",
        "            return float(value.replace('k', '')) * 1000\n",
        "        elif 'm' in value:\n",
        "            return float(value.replace('m', '')) * 1000000\n",
        "        elif 'b'in value:\n",
        "            return float(value.replace('b', '')) * 1000000000\n",
        "\n",
        "    return value\n",
        "\n",
        "# Apply the conversion function to numerical columns\n",
        "for column in ['posts', 'followers', 'avg_likes', 'new_post_avg_like', 'total_likes', 'total_likes']:\n",
        "    df[column] = df[column].apply(convert_abbreviated_numbers)\n",
        "\n",
        "# Normalize numerical columns\n",
        "def normalize_features(df, columns):\n",
        "    scaler = MinMaxScaler()\n",
        "    df[columns] = scaler.fit_transform(df[columns])\n",
        "    return df\n",
        "\n",
        "numerical_columns = ['posts', 'followers', 'avg_likes', 'new_post_avg_like', 'total_likes']\n",
        "df = normalize_features(df, numerical_columns)\n",
        "\n",
        "# ... (rest of the code remains the same) ...\n",
        "\n",
        "# Define input features and target variable\n",
        "X_data = df[['posts', 'followers', 'avg_likes', 'new_post_avg_like', 'total_likes']].values\n",
        "y_data = df['total_likes'].values  # Using total likes as a proxy for influence ranking\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
        "    X_data, y_data, np.arange(len(df)), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the TH-DCNN model architecture\n",
        "def create_th_dcnn_model(input_shape):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # DCNN layers\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(input_shape[0], 1)))\n",
        "    model.add(layers.MaxPooling1D(pool_size=2))\n",
        "    # Reduced kernel_size and added padding\n",
        "    model.add(layers.Conv1D(filters=128, kernel_size=2, activation='relu', padding='same'))\n",
        "    # Change pool_size to 1 to avoid negative dimension\n",
        "    model.add(layers.MaxPooling1D(pool_size=1))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='linear'))  # For regression (influence score)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Reshape input for Conv1D\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Create the model\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "model = create_th_dcnn_model(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n",
        "\n",
        "# Rank influencers by the predicted influence score\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Combine the results with the original dataset for ranking\n",
        "df_test = df.iloc[test_indices]  # Get corresponding rows from the original DataFrame for test set\n",
        "df_test['predicted_influence'] = y_pred\n",
        "\n",
        "# Rank influencers by the predicted influence score\n",
        "df_test['rank'] = df_test['predicted_influence'].rank(ascending=False)\n",
        "\n",
        "# Display top-ranked influencers based on the predicted influence score\n",
        "print(df_test[['channel_info', 'predicted_influence', 'rank', 'country']].head())\n"
      ]
    }
  ]
}